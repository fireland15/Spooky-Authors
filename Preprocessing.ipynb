{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox\n",
    "\n",
    "Here is the sandbox where we tried many different thing to determine what we wanted to do for our submissions. This includes only some of the many combiniations of networks and other techniques we attempted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Beth &\n",
      "[nltk_data]     Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Beth &\n",
      "[nltk_data]     Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some methods that make life easier as we go along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "    labels = np.array([a2c[a] for a in data.author])\n",
    "    labels = to_categorical(labels)\n",
    "    return labels\n",
    "\n",
    "def get_text_only(data):\n",
    "    return data[\"text\"]\n",
    "\n",
    "def calc_metrics(x, y_true, y_pred):\n",
    "    return classification_report(y_true, y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calc_confusion_matrix(y_true, y_pred):\n",
    "    return confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function defines our machine learning pipeline that we can add and remove peices from to easily test many different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(load_func, preprocess_func, create_model_func, verbosity=2):\n",
    "    print(\"Loading data\")\n",
    "    full_data = load_func()\n",
    "    \n",
    "    print(\"Getting labels\")\n",
    "    labels = make_labels(full_data)    \n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    data = preprocess_func(get_text_only(full_data))\n",
    "    \n",
    "    input_dim = max([max(x) for x in data]) + 1\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    model = create_model_func(input_dim)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    \n",
    "    print(\"Training model\")\n",
    "    model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=64,\n",
    "                 verbose=verbosity,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    print(\"Testing model\")\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    y_pred = to_categorical(y_pred, num_classes=3)\n",
    "    \n",
    "    print(\"Test results\")  \n",
    "    print(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    print(\"metrics\")\n",
    "    print(calc_metrics(x_test, y_test, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(calc_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    return pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes stop words from the sentences in text\n",
    "def remove_stops(text):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [\" \".join([word for word in nltk.word_tokenize(words) if word not in stops]) for words in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the sentences in text into a sequence of numbers\n",
    "def convert_to_sequences(text, filters, to_lower):\n",
    "    tokenizer = Tokenizer(filters=filters, lower=to_lower, split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "def convert_to_word_sequence(text):\n",
    "    return [text_to_word_sequence(words) for words in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(text):\n",
    "    maxlen = np.amax([len(x) for x in text], axis=0)\n",
    "    return pad_sequences(sequences=text, maxlen=maxlen)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converts each text into a sequence of number and then pads so all sequences have identical length\n",
    "def convert_to_sequence_and_pad(text):\n",
    "    return pad_data(convert_to_sequences(text, \"\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model(input_dim, embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.831460674157\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.91      0.84      1583\n",
      "          1       0.92      0.75      0.83      1130\n",
      "          2       0.85      0.81      0.83      1203\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1438   43  102]\n",
      " [ 216  848   66]\n",
      " [ 207   26  970]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our first run at a simple model. The accuracy is 82%, which is pretty good for a first time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\n",
      "After:   [ 6  7  1  2  3  4  5  8  9 10 11 12 13  5  2  3  4 14 15 16  1]\n"
     ]
    }
   ],
   "source": [
    "# 1. Filters certain characters (~!@#$%^&*()_+`-=,./;'<>?:\") from the text\n",
    "# 2. converts each text into a sequence of numbers\n",
    "# 3. pads each sequence to be the same length\n",
    "def convert_to_sequence_and_pad_and_filter_chars(text):\n",
    "    return pad_data(convert_to_sequences(text, \"~!@#$%^&*()_+`-=,./;'<>?:\\\"\", False))\n",
    "\n",
    "sample = [\"The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After:  \", convert_to_sequence_and_pad_and_filter_chars(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.845505617978\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.89      0.85      1567\n",
      "          1       0.86      0.86      0.86      1139\n",
      "          2       0.89      0.78      0.83      1210\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1393   92   82]\n",
      " [ 132  975   32]\n",
      " [ 197   70  943]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad_and_filter_chars, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tried the same model, but with a different pre-processing step, this increased our accuracy a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\n",
      "After:  [ 5  6  1  2  3  7  4  8  9 10 11  2  3  4  1 12]\n"
     ]
    }
   ],
   "source": [
    "# 1. Removes stopwords using the nltk supplied stop words for english\n",
    "# 2. Converts the texts to sequences of numbers\n",
    "# 3. Pads the sequences to identical lengths\n",
    "def remove_stopwords_then_convert_to_sequence_and_pad(text):\n",
    "    return pad_data(convert_to_sequences(remove_stops(text), \"\", False))\n",
    "\n",
    "sample = [\"The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After: \", remove_stopwords_then_convert_to_sequence_and_pad(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.836057201226\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.90      0.84      1580\n",
      "          1       0.87      0.81      0.84      1106\n",
      "          2       0.87      0.78      0.82      1230\n",
      "\n",
      "avg / total       0.84      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1420   70   90]\n",
      " [ 161  897   48]\n",
      " [ 211   62  957]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, remove_stopwords_then_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stopwords, our mode has an accuracy of 84%, its getting better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\n",
      "After:  [ 6  1  7  8  9  2  1  3  4  5 10  1 11 12 13 14 15 16  5  2  3  4 17 18 19\n",
      " 20 21]\n"
     ]
    }
   ],
   "source": [
    "# We had seen a submission where the person treated punctuation as distinct words, we thought this would be worth trying\n",
    "#  seeing as some authors may have different patterns of punctuation\n",
    "def convert_punctuation_to_words(text):\n",
    "    t = text.replace(\",\", \" , \")\n",
    "    t = t.replace(\".\", \" . \")\n",
    "    t = t.replace(\"'\", \" ' \")\n",
    "    t = t.replace(\";\", \" ; \")\n",
    "    t = t.replace(\":\", \" : \")\n",
    "    return t\n",
    "\n",
    "# 1. Introduces spacing to make punctuation a distinct word\n",
    "# 2. converts each text into a sequence of numbers\n",
    "# 3. pads the data to the same length\n",
    "def make_punctuation_words_and_convert_to_sequence(text):\n",
    "    t = [convert_punctuation_to_words(x) for x in text]\n",
    "    return pad_data(convert_to_sequences(t, \"\", False))\n",
    "\n",
    "sample = [\"Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After: \", make_punctuation_words_and_convert_to_sequence(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.841675178754\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.92      0.84      1547\n",
      "          1       0.88      0.84      0.86      1112\n",
      "          2       0.92      0.74      0.82      1257\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1430   60   57]\n",
      " [ 156  935   21]\n",
      " [ 264   62  931]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, make_punctuation_words_and_convert_to_sequence, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no large change in the accuracy when we treated punctation as words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\n",
      "After:  [ 6  1  7  8  9  2  1  3  4  5 10  1 11 12 13 14 15 16  5  2  3  4 17 18 19\n",
      " 20 21]\n"
     ]
    }
   ],
   "source": [
    "# 1. Converts each punctuation to a separte word\n",
    "# 2. removes the stop words from each text\n",
    "# 3. converts the texts into sequences of numbers\n",
    "# 4. Pads each sequence to the same length\n",
    "def punctuation_as_words_and_remove_stopwords(text):\n",
    "    t = convert_punctuation_to_words(text)\n",
    "    t = remove_stops(t)\n",
    "    return pad_data(convert_to_sequences(t, \"\", False))\n",
    "\n",
    "sample = [\"Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After: \", make_punctuation_words_and_convert_to_sequence(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.834780388151\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.94      0.84      1599\n",
      "          1       0.93      0.78      0.85      1114\n",
      "          2       0.89      0.74      0.81      1203\n",
      "\n",
      "avg / total       0.85      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1499   34   66]\n",
      " [ 196  874   44]\n",
      " [ 275   32  896]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, punctuation_as_words_and_remove_stopwords, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy decreased when we treated punctucation as words and removed stop words. Maybe we are not distiling our the correct feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the simple model, but decreases the embedding dimensions to 10\n",
    "def create_simple_model_with_fewer_embedding_dims(input_dim):\n",
    "    return create_simple_model(input_dim, embedding_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.830949948927\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.86      0.84      1599\n",
      "          1       0.89      0.78      0.83      1092\n",
      "          2       0.81      0.84      0.82      1225\n",
      "\n",
      "avg / total       0.83      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1376   64  159]\n",
      " [ 158  852   82]\n",
      " [ 158   41 1026]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model_with_fewer_embedding_dims, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the simple model, but increases the embedding dimensions to 30\n",
    "def create_simple_model_with_more_embedding_dims(input_dim):\n",
    "    return create_simple_model(input_dim, embedding_dims=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.821756894791\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.87      0.83      1583\n",
      "          1       0.79      0.87      0.83      1145\n",
      "          2       0.90      0.72      0.80      1188\n",
      "\n",
      "avg / total       0.83      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1374  142   67]\n",
      " [ 124  991   30]\n",
      " [ 219  116  853]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model_with_more_embedding_dims, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more emdedding demensions decreased our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Does some lemmatizing on  the texts to hopefully make similar words the same\n",
    "# set to prefer the verb version of words if there is are conflicting choices\n",
    "def lemmatize_texts_to_verbs(texts):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    return [\" \".join([lmtzr.lemmatize(word, \"v\") for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "\n",
    "# 1. Lemmatizes each text\n",
    "# 2. Converts each text into a sequence of numbers and pads the seqeunces to the same length\n",
    "def lemmatize_verb_and_convert_to_sequence_and_pad(text):\n",
    "    t = lemmatize_texts_to_verbs(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.850102145046\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.90      0.86      1621\n",
      "          1       0.94      0.79      0.86      1133\n",
      "          2       0.83      0.84      0.84      1162\n",
      "\n",
      "avg / total       0.86      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1459   38  124]\n",
      " [ 165  896   72]\n",
      " [ 164   24  974]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, lemmatize_verb_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Does some lemmatizing on  the texts to hopefully make similar words the same\n",
    "# set to prefer the verb version of words if there is are conflicting choices\n",
    "def lemmatize_texts_to_nouns(texts):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    return [\" \".join([lmtzr.lemmatize(word, \"n\") for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "\n",
    "# 1. Lemmatizes each text\n",
    "# 2. Converts each text into a sequence of numbers and pads the seqeunces to the same length\n",
    "def lemmatize_noun_and_convert_to_sequence_and_pad(text):\n",
    "    t = lemmatize_texts_to_nouns(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.820224719101\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.89      0.83      1593\n",
      "          1       0.94      0.72      0.82      1193\n",
      "          2       0.81      0.82      0.82      1130\n",
      "\n",
      "avg / total       0.83      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1418   42  133]\n",
      " [ 242  863   88]\n",
      " [ 183   16  931]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, lemmatize_noun_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of our pre-processing step do not seem to be making much of an impact, or are making things worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uses the Porter stemmer to stem each word in the texts\n",
    "def stem_texts_porter(texts):\n",
    "    stmr = PorterStemmer()\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "    \n",
    "# 1. Stems texts with the porter stemmer\n",
    "# 2. Convert texts into sequences of numbers and pads those\n",
    "def stem_porter_and_convert_to_sequence_and_pad(text):\n",
    "    t = stem_texts_porter(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.850102145046\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.81      0.85      1557\n",
      "          1       0.86      0.87      0.86      1143\n",
      "          2       0.80      0.88      0.84      1216\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1260  107  190]\n",
      " [  74  993   76]\n",
      " [  84   56 1076]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, stem_porter_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the snowball stemmer to stem words in texts\n",
    "def stem_texts_snowball(texts):\n",
    "    stmr = SnowballStemmer(\"english\")\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "  \n",
    "# 1. stems words in texts with the snowball stemmer\n",
    "# 2. converts texts to sequences of numbers and pads those\n",
    "def stem_snowball_and_convert_to_sequence_and_pad(text):\n",
    "    t = stem_texts_snowball(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.847803881512\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.87      0.85      1560\n",
      "          1       0.84      0.88      0.86      1147\n",
      "          2       0.87      0.80      0.83      1209\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1350  114   96]\n",
      " [ 100 1004   43]\n",
      " [ 163   80  966]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, stem_snowball_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes words from each sequence where the filter_func returns false\n",
    "def filter_words(sequences, filter_func):\n",
    "    return [[word for word in sequence if filter_func(word) is not False] for sequence in sequences]\n",
    "\n",
    "def create_infrequent_words_filter(num_to_keep):\n",
    "    def f(text):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        t = convert_to_word_sequence(text)\n",
    "        t = filter_words(t, lambda w: tokenizer.word_index[w] > num_to_keep)\n",
    "        return [[tokenizer.word_index[word] for word in seq] for seq in t]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.827630234934\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.91      0.84      1608\n",
      "          1       0.90      0.76      0.82      1152\n",
      "          2       0.84      0.78      0.81      1156\n",
      "\n",
      "avg / total       0.83      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1462   55   91]\n",
      " [ 197  872   83]\n",
      " [ 205   44  907]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(100)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.778855975485\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.75      0.78      1600\n",
      "          1       0.84      0.76      0.80      1111\n",
      "          2       0.70      0.84      0.76      1205\n",
      "\n",
      "avg / total       0.79      0.78      0.78      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1195  108  297]\n",
      " [ 130  841  140]\n",
      " [ 136   55 1014]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(1000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the most frequent words was not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.516598569969\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.90      0.62      1574\n",
      "          1       0.72      0.34      0.46      1169\n",
      "          2       0.56      0.18      0.27      1173\n",
      "\n",
      "avg / total       0.57      0.52      0.47      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1414   82   78]\n",
      " [ 686  398   85]\n",
      " [ 886   76  211]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(10000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequent_words_filter(num_to_keep):\n",
    "    def f(text):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        t = convert_to_word_sequence(text)\n",
    "        t = filter_words(t, lambda w: tokenizer.word_index[w] <= num_to_keep)\n",
    "        return [[tokenizer.word_index[word] for word in seq] for seq in t]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.577374872319\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.65      1565\n",
      "          1       0.64      0.36      0.46      1152\n",
      "          2       0.58      0.54      0.56      1199\n",
      "\n",
      "avg / total       0.59      0.58      0.56      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1197  120  248]\n",
      " [ 510  416  226]\n",
      " [ 434  117  648]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(100)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neither was removing the least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.728038815117\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.73      0.74      1609\n",
      "          1       0.63      0.81      0.71      1068\n",
      "          2       0.81      0.66      0.73      1239\n",
      "\n",
      "avg / total       0.74      0.73      0.73      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1168  305  136]\n",
      " [ 145  861   62]\n",
      " [ 218  199  822]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(1000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.813329928498\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.86      0.82      1573\n",
      "          1       0.88      0.74      0.81      1131\n",
      "          2       0.81      0.82      0.82      1212\n",
      "\n",
      "avg / total       0.82      0.81      0.81      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1350   73  150]\n",
      " [ 207  836   88]\n",
      " [ 176   37  999]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(5000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.833758937692\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.91      0.84      1596\n",
      "          1       0.87      0.79      0.83      1098\n",
      "          2       0.91      0.78      0.84      1222\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1453   82   61]\n",
      " [ 198  863   37]\n",
      " [ 227   46  949]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(10000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.828907048008\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.92      0.83      1524\n",
      "          1       0.86      0.82      0.84      1154\n",
      "          2       0.92      0.73      0.81      1238\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1396   81   47]\n",
      " [ 175  943   36]\n",
      " [ 264   67  907]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(15000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we were very intrested to try out was a knn classifier. However, it was just too much for our machines. Even running on the GPU, it was taking 20 minuets and using 10 GB of memory. It was a cool idea, but didnt work very well at all with super low accuracy\n",
    "\n",
    "example knn classifier: https://stackoverflow.com/questions/42872425/text-classification-using-knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocess_func_arr is an array of functions to preprocess the text. The func will be called in order and the \n",
    "#   output of each will be the input to the next\n",
    "def run_knn(load_func, preprocess_func_arr, create_model_func):\n",
    "    print(\"Loading data\")\n",
    "    full_data = load_func()\n",
    "    \n",
    "    print(\"Getting labels\")\n",
    "    labels = make_labels(full_data)    \n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    data = get_text_only(full_data)\n",
    "    for func in preprocess_func_arr:\n",
    "        data = func(data)\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    model = create_model_func()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    print(y_test[0])\n",
    "    print(\"Training model\")\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    print(\"Testing model\")\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(y_pred[0])\n",
    "    \n",
    "    print(\"Test results\")  \n",
    "    print(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    print(\"metrics\")\n",
    "    print(calc_metrics(x_test, y_test, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(calc_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knn_model():\n",
    "    return KNeighborsClassifier(n_neighbors=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_one_hot(text):\n",
    "    print(text[0])\n",
    "    vectorizer = CountVectorizer()\n",
    "    text = vectorizer.fit_transform(text).todense()\n",
    "    print(text[0])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "[[0 0 0 ..., 0 0 0]]\n",
      "Creating model\n",
      "[ 1.  0.  0.]\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "[ 0.  0.  1.]\n",
      "Test results\n",
      "accuracy 0.415730337079\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.73      0.57      1577\n",
      "          1       0.62      0.08      0.14      1096\n",
      "          2       0.55      0.32      0.40      1243\n",
      "\n",
      "avg / total       0.53      0.42      0.39      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1345   26  206]\n",
      " [ 893   84  119]\n",
      " [ 824   26  393]]\n"
     ]
    }
   ],
   "source": [
    "run_knn(load_training_data, [convert_words_to_one_hot], create_knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "This process , however , afforded means ascertaining dimensions dungeon ; I might make circuit , return point whence I set , without aware fact ; perfectly uniform seemed wall .\n",
      "[[0 0 0 ..., 0 0 0]]\n",
      "Creating model\n",
      "[ 1.  0.  0.]\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "[ 0.  0.  1.]\n",
      "Test results\n",
      "accuracy 0.378958120531\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.55      0.51      1620\n",
      "          1       0.50      0.04      0.07      1108\n",
      "          2       0.38      0.47      0.42      1188\n",
      "\n",
      "avg / total       0.45      0.38      0.36      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1050   19  551]\n",
      " [ 716   39  353]\n",
      " [ 613   20  555]]\n"
     ]
    }
   ],
   "source": [
    "run_knn(load_training_data, [remove_stops, convert_words_to_one_hot], create_knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

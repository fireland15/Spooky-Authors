{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def make_labels(data):\n",
    "    a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "    labels = np.array([a2c[a] for a in data.author])\n",
    "    labels = to_categorical(labels)\n",
    "    return labels\n",
    "\n",
    "def get_text_only(data):\n",
    "    return data[\"text\"]\n",
    "\n",
    "def calc_metrics(x, y_true, y_pred):\n",
    "    return classification_report(y_true, y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calc_confusion_matrix(y_true, y_pred):\n",
    "    return confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def run(load_func, preprocess_func, create_model_func, verbosity=2):\n",
    "    print(\"Loading data\")\n",
    "    full_data = load_func()\n",
    "    \n",
    "    print(\"Getting labels\")\n",
    "    labels = make_labels(full_data)    \n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    data = preprocess_func(get_text_only(full_data))\n",
    "    \n",
    "    input_dim = max([max(x) for x in data]) + 1\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    model = create_model_func(input_dim)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    \n",
    "    print(\"Training model\")\n",
    "    model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=64,\n",
    "                 verbose=verbosity,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    print(\"Testing model\")\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    y_pred = to_categorical(y_pred, num_classes=3)\n",
    "    \n",
    "    print(\"Test results\")  \n",
    "    print(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    print(\"metrics\")\n",
    "    print(calc_metrics(x_test, y_test, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(calc_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_training_data():\n",
    "    return pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_simple_model(input_dim, embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# removes stop words from the sentences in text\n",
    "def remove_stops(text):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [\" \".join([word for word in nltk.word_tokenize(words) if word not in stops]) for words in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# converts the sentences in text into a sequence of numbers\n",
    "def convert_to_sequences(text, filters, to_lower):\n",
    "    tokenizer = Tokenizer(filters=filters, lower=to_lower, split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def convert_to_word_sequence(text):\n",
    "    return [text_to_word_sequence(words) for words in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_data(text):\n",
    "    maxlen = np.amax([len(x) for x in text], axis=0)\n",
    "    return pad_sequences(sequences=text, maxlen=maxlen)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converts each text into a sequence of number and then pads so all sequences have identical length\n",
    "def convert_to_sequence_and_pad(text):\n",
    "    return pad_data(convert_to_sequences(text, \"\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.823033707865\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.79      0.83      1631\n",
      "          1       0.76      0.88      0.82      1095\n",
      "          2       0.83      0.81      0.82      1190\n",
      "\n",
      "avg / total       0.83      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1293  190  148]\n",
      " [  75  964   56]\n",
      " [ 113  111  966]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\n",
      "After:   [ 6  7  1  2  3  4  5  8  9 10 11 12 13  5  2  3  4 14 15 16  1]\n"
     ]
    }
   ],
   "source": [
    "# 1. Filters certain characters (~!@#$%^&*()_+`-=,./;'<>?:\") from the text\n",
    "# 2. converts each text into a sequence of numbers\n",
    "# 3. pads each sequence to be the same length\n",
    "def convert_to_sequence_and_pad_and_filter_chars(text):\n",
    "    return pad_data(convert_to_sequences(text, \"~!@#$%^&*()_+`-=,./;'<>?:\\\"\", False))\n",
    "\n",
    "sample = [\"The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After:  \", convert_to_sequence_and_pad_and_filter_chars(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.83835546476\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.83      0.84      1553\n",
      "          1       0.94      0.77      0.84      1168\n",
      "          2       0.76      0.91      0.83      1195\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1291   43  219]\n",
      " [ 135  899  134]\n",
      " [  83   19 1093]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad_and_filter_chars, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\n",
      "After:  [ 5  6  1  2  3  7  4  8  9 10 11  2  3  4  1 12]\n"
     ]
    }
   ],
   "source": [
    "# 1. Removes stopwords using the nltk supplied stop words for english\n",
    "# 2. Converts the texts to sequences of numbers\n",
    "# 3. Pads the sequences to identical lengths\n",
    "def remove_stopwords_then_convert_to_sequence_and_pad(text):\n",
    "    return pad_data(convert_to_sequences(remove_stops(text), \"\", False))\n",
    "\n",
    "sample = [\"The quick brown fox jumped over the something, I can't remember what the fox jumped over, but it was brown.\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After: \", remove_stopwords_then_convert_to_sequence_and_pad(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.841164453524\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.91      0.84      1598\n",
      "          1       0.89      0.79      0.84      1089\n",
      "          2       0.89      0.79      0.84      1229\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1456   66   76]\n",
      " [ 183  861   45]\n",
      " [ 210   42  977]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, remove_stopwords_then_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\n",
      "After:  [ 6  1  7  8  9  2  1  3  4  5 10  1 11 12 13 14 15 16  5  2  3  4 17 18 19\n",
      " 20 21]\n"
     ]
    }
   ],
   "source": [
    "# We had seen a submission where the person treated punctuation as distinct words, we thought this would be worth trying\n",
    "#  seeing as some authors may have different patterns of punctuation\n",
    "def convert_punctuation_to_words(text):\n",
    "    t = text.replace(\",\", \" , \")\n",
    "    t = t.replace(\".\", \" . \")\n",
    "    t = t.replace(\"'\", \" ' \")\n",
    "    t = t.replace(\";\", \" ; \")\n",
    "    t = t.replace(\":\", \" : \")\n",
    "    return t\n",
    "\n",
    "# 1. Introduces spacing to make punctuation a distinct word\n",
    "# 2. converts each text into a sequence of numbers\n",
    "# 3. pads the data to the same length\n",
    "def make_punctuation_words_and_convert_to_sequence(text):\n",
    "    t = [convert_punctuation_to_words(x) for x in text]\n",
    "    return pad_data(convert_to_sequences(t, \"\", False))\n",
    "\n",
    "sample = [\"Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After: \", make_punctuation_words_and_convert_to_sequence(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.848314606742\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.82      0.85      1586\n",
      "          1       0.83      0.86      0.85      1115\n",
      "          2       0.83      0.87      0.85      1215\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1297  139  150]\n",
      " [  90  962   63]\n",
      " [  95   57 1063]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, make_punctuation_words_and_convert_to_sequence, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\n",
      "After:  [ 6  1  7  8  9  2  1  3  4  5 10  1 11 12 13 14 15 16  5  2  3  4 17 18 19\n",
      " 20 21]\n"
     ]
    }
   ],
   "source": [
    "# 1. Converts each punctuation to a separte word\n",
    "# 2. removes the stop words from each text\n",
    "# 3. converts the texts into sequences of numbers\n",
    "# 4. Pads each sequence to the same length\n",
    "def punctuation_as_words_and_remove_stopwords(text):\n",
    "    t = convert_punctuation_to_words(text)\n",
    "    t = remove_stops(t)\n",
    "    return pad_data(convert_to_sequences(t, \"\", False))\n",
    "\n",
    "sample = [\"Hello - The quick brown fox - jumped over the something - I can't remember what the fox jumped over, but it was brown!\"]\n",
    "print(\"Before: \", sample[0])\n",
    "print(\"After: \", make_punctuation_words_and_convert_to_sequence(sample)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.834780388151\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.94      0.84      1599\n",
      "          1       0.93      0.78      0.85      1114\n",
      "          2       0.89      0.74      0.81      1203\n",
      "\n",
      "avg / total       0.85      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1499   34   66]\n",
      " [ 196  874   44]\n",
      " [ 275   32  896]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, punctuation_as_words_and_remove_stopwords, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the simple model, but decreases the embedding dimensions to 10\n",
    "def create_simple_model_with_fewer_embedding_dims(input_dim):\n",
    "    return create_simple_model(input_dim, embedding_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.833758937692\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.91      0.84      1593\n",
      "          1       0.92      0.73      0.82      1119\n",
      "          2       0.84      0.83      0.84      1204\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1445   45  103]\n",
      " [ 217  818   84]\n",
      " [ 179   23 1002]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model_with_fewer_embedding_dims, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the simple model, but increases the embedding dimensions to 30\n",
    "def create_simple_model_with_more_embedding_dims(input_dim):\n",
    "    return create_simple_model(input_dim, embedding_dims=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.821756894791\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.87      0.83      1583\n",
      "          1       0.79      0.87      0.83      1145\n",
      "          2       0.90      0.72      0.80      1188\n",
      "\n",
      "avg / total       0.83      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1374  142   67]\n",
      " [ 124  991   30]\n",
      " [ 219  116  853]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model_with_more_embedding_dims, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Does some lemmatizing on  the texts to hopefully make similar words the same\n",
    "# set to prefer the verb version of words if there is are conflicting choices\n",
    "def lemmatize_texts_to_verbs(texts):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    return [\" \".join([lmtzr.lemmatize(word, \"v\") for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "\n",
    "# 1. Lemmatizes each text\n",
    "# 2. Converts each text into a sequence of numbers and pads the seqeunces to the same length\n",
    "def lemmatize_verb_and_convert_to_sequence_and_pad(text):\n",
    "    t = lemmatize_texts_to_verbs(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.850102145046\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.90      0.86      1621\n",
      "          1       0.94      0.79      0.86      1133\n",
      "          2       0.83      0.84      0.84      1162\n",
      "\n",
      "avg / total       0.86      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1459   38  124]\n",
      " [ 165  896   72]\n",
      " [ 164   24  974]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, lemmatize_verb_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Does some lemmatizing on  the texts to hopefully make similar words the same\n",
    "# set to prefer the verb version of words if there is are conflicting choices\n",
    "def lemmatize_texts_to_nouns(texts):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    return [\" \".join([lmtzr.lemmatize(word, \"n\") for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "\n",
    "# 1. Lemmatizes each text\n",
    "# 2. Converts each text into a sequence of numbers and pads the seqeunces to the same length\n",
    "def lemmatize_noun_and_convert_to_sequence_and_pad(text):\n",
    "    t = lemmatize_texts_to_nouns(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.820224719101\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.89      0.83      1593\n",
      "          1       0.94      0.72      0.82      1193\n",
      "          2       0.81      0.82      0.82      1130\n",
      "\n",
      "avg / total       0.83      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1418   42  133]\n",
      " [ 242  863   88]\n",
      " [ 183   16  931]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, lemmatize_noun_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Uses the Porter stemmer to stem each word in the texts\n",
    "def stem_texts_porter(texts):\n",
    "    stmr = PorterStemmer()\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "    \n",
    "# 1. Stems texts with the porter stemmer\n",
    "# 2. Convert texts into sequences of numbers and pads those\n",
    "def stem_porter_and_convert_to_sequence_and_pad(text):\n",
    "    t = stem_texts_porter(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.850102145046\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.81      0.85      1557\n",
      "          1       0.86      0.87      0.86      1143\n",
      "          2       0.80      0.88      0.84      1216\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1260  107  190]\n",
      " [  74  993   76]\n",
      " [  84   56 1076]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, stem_porter_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Uses the snowball stemmer to stem words in texts\n",
    "def stem_texts_snowball(texts):\n",
    "    stmr = SnowballStemmer(\"english\")\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "  \n",
    "# 1. stems words in texts with the snowball stemmer\n",
    "# 2. converts texts to sequences of numbers and pads those\n",
    "def stem_snowball_and_convert_to_sequence_and_pad(text):\n",
    "    t = stem_texts_snowball(text)\n",
    "    return convert_to_sequence_and_pad(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.847803881512\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.87      0.85      1560\n",
      "          1       0.84      0.88      0.86      1147\n",
      "          2       0.87      0.80      0.83      1209\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1350  114   96]\n",
      " [ 100 1004   43]\n",
      " [ 163   80  966]]\n"
     ]
    }
   ],
   "source": [
    "run(load_training_data, stem_snowball_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "# removes words from each sequence where the filter_func returns false\n",
    "def filter_words(sequences, filter_func):\n",
    "    return [[word for word in sequence if filter_func(word) is not False] for sequence in sequences]\n",
    "\n",
    "def create_infrequent_words_filter(num_to_keep):\n",
    "    def f(text):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        t = convert_to_word_sequence(text)\n",
    "        t = filter_words(t, lambda w: tokenizer.word_index[w] > num_to_keep)\n",
    "        return [[tokenizer.word_index[word] for word in seq] for seq in t]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.827630234934\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.91      0.84      1608\n",
      "          1       0.90      0.76      0.82      1152\n",
      "          2       0.84      0.78      0.81      1156\n",
      "\n",
      "avg / total       0.83      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1462   55   91]\n",
      " [ 197  872   83]\n",
      " [ 205   44  907]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(100)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.778855975485\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.75      0.78      1600\n",
      "          1       0.84      0.76      0.80      1111\n",
      "          2       0.70      0.84      0.76      1205\n",
      "\n",
      "avg / total       0.79      0.78      0.78      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1195  108  297]\n",
      " [ 130  841  140]\n",
      " [ 136   55 1014]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(1000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.516598569969\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.90      0.62      1574\n",
      "          1       0.72      0.34      0.46      1169\n",
      "          2       0.56      0.18      0.27      1173\n",
      "\n",
      "avg / total       0.57      0.52      0.47      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1414   82   78]\n",
      " [ 686  398   85]\n",
      " [ 886   76  211]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(10000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "def create_frequent_words_filter(num_to_keep):\n",
    "    def f(text):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        t = convert_to_word_sequence(text)\n",
    "        t = filter_words(t, lambda w: tokenizer.word_index[w] <= num_to_keep)\n",
    "        return [[tokenizer.word_index[word] for word in seq] for seq in t]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.577374872319\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.65      1565\n",
      "          1       0.64      0.36      0.46      1152\n",
      "          2       0.58      0.54      0.56      1199\n",
      "\n",
      "avg / total       0.59      0.58      0.56      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1197  120  248]\n",
      " [ 510  416  226]\n",
      " [ 434  117  648]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(100)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.728038815117\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.73      0.74      1609\n",
      "          1       0.63      0.81      0.71      1068\n",
      "          2       0.81      0.66      0.73      1239\n",
      "\n",
      "avg / total       0.74      0.73      0.73      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1168  305  136]\n",
      " [ 145  861   62]\n",
      " [ 218  199  822]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(1000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.813329928498\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.86      0.82      1573\n",
      "          1       0.88      0.74      0.81      1131\n",
      "          2       0.81      0.82      0.82      1212\n",
      "\n",
      "avg / total       0.82      0.81      0.81      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1350   73  150]\n",
      " [ 207  836   88]\n",
      " [ 176   37  999]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(5000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.833758937692\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.91      0.84      1596\n",
      "          1       0.87      0.79      0.83      1098\n",
      "          2       0.91      0.78      0.84      1222\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1453   82   61]\n",
      " [ 198  863   37]\n",
      " [ 227   46  949]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(10000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.828907048008\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.92      0.83      1524\n",
      "          1       0.86      0.82      0.84      1154\n",
      "          2       0.92      0.73      0.81      1238\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1396   81   47]\n",
      " [ 175  943   36]\n",
      " [ 264   67  907]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(15000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example knn classifier: https://stackoverflow.com/questions/42872425/text-classification-using-knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# The preprocess_func_arr is an array of functions to preprocess the text. The func will be called in order and the \n",
    "#   output of each will be the input to the next\n",
    "def run_knn(load_func, preprocess_func_arr, create_model_func):\n",
    "    print(\"Loading data\")\n",
    "    full_data = load_func()\n",
    "    \n",
    "    print(\"Getting labels\")\n",
    "    labels = make_labels(full_data)    \n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    data = get_text_only(full_data)\n",
    "    for func in preprocess_func_arr:\n",
    "        data = func(data)\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    model = create_model_func()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    print(y_test[0])\n",
    "    print(\"Training model\")\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    print(\"Testing model\")\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(y_pred[0])\n",
    "    \n",
    "    print(\"Test results\")  \n",
    "    print(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    print(\"metrics\")\n",
    "    print(calc_metrics(x_test, y_test, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(calc_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def create_knn_model():\n",
    "    return KNeighborsClassifier(n_neighbors=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def convert_words_to_one_hot(text):\n",
    "    print(text[0])\n",
    "    vectorizer = CountVectorizer()\n",
    "    text = vectorizer.fit_transform(text).todense()\n",
    "    print(text[0])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "[[0 0 0 ..., 0 0 0]]\n",
      "Creating model\n",
      "[ 1.  0.  0.]\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "[ 0.  0.  1.]\n",
      "Test results\n",
      "accuracy 0.415730337079\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.73      0.57      1577\n",
      "          1       0.62      0.08      0.14      1096\n",
      "          2       0.55      0.32      0.40      1243\n",
      "\n",
      "avg / total       0.53      0.42      0.39      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1345   26  206]\n",
      " [ 893   84  119]\n",
      " [ 824   26  393]]\n"
     ]
    }
   ],
   "source": [
    "run_knn(load_training_data, [convert_words_to_one_hot], create_knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "This process , however , afforded means ascertaining dimensions dungeon ; I might make circuit , return point whence I set , without aware fact ; perfectly uniform seemed wall .\n",
      "[[0 0 0 ..., 0 0 0]]\n",
      "Creating model\n",
      "[ 1.  0.  0.]\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "[ 0.  0.  1.]\n",
      "Test results\n",
      "accuracy 0.378958120531\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.55      0.51      1620\n",
      "          1       0.50      0.04      0.07      1108\n",
      "          2       0.38      0.47      0.42      1188\n",
      "\n",
      "avg / total       0.45      0.38      0.36      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1050   19  551]\n",
      " [ 716   39  353]\n",
      " [ 613   20  555]]\n"
     ]
    }
   ],
   "source": [
    "run_knn(load_training_data, [remove_stops, convert_words_to_one_hot], create_knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

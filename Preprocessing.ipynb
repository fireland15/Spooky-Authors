{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def make_labels(data):\n",
    "    a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "    labels = np.array([a2c[a] for a in data.author])\n",
    "    labels = to_categorical(labels)\n",
    "    return labels\n",
    "\n",
    "def get_text_only(data):\n",
    "    return data[\"text\"]\n",
    "\n",
    "def calc_metrics(x, y_true, y_pred):\n",
    "    return classification_report(y_true, y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calc_confusion_matrix(y_true, y_pred):\n",
    "    return confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def run(load_func, preprocess_func, create_model_func, verbosity=2):\n",
    "    print(\"Loading data\")\n",
    "    full_data = load_func()\n",
    "    \n",
    "    print(\"Getting labels\")\n",
    "    labels = make_labels(full_data)    \n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    data = preprocess_func(get_text_only(full_data))\n",
    "    \n",
    "    input_dim = max([max(x) for x in data]) + 1\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    model = create_model_func(input_dim)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    \n",
    "    print(\"Training model\")\n",
    "    model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=64,\n",
    "                 verbose=verbosity,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    print(\"Testing model\")\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    y_pred = to_categorical(y_pred, num_classes=3)\n",
    "    \n",
    "    print(\"Test results\")  \n",
    "    print(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    print(\"metrics\")\n",
    "    print(calc_metrics(x_test, y_test, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(calc_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_training_data():\n",
    "    return pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_simple_model(input_dim, embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# removes stop words from the sentences in text\n",
    "def remove_stops(text):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [\" \".join([word for word in nltk.word_tokenize(words) if word not in stops]) for words in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# converts the sentences in text into a sequence of numbers\n",
    "def convert_to_sequences(text, filters, to_lower):\n",
    "    tokenizer = Tokenizer(filters=filters, lower=to_lower, split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def convert_to_word_sequence(text):\n",
    "    return [text_to_word_sequence(words) for words in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_data(text):\n",
    "    maxlen = np.amax([len(x) for x in text], axis=0)\n",
    "    return pad_sequences(sequences=text, maxlen=maxlen)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.831716036772\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.87      0.84      1586\n",
      "          1       0.91      0.75      0.82      1115\n",
      "          2       0.80      0.86      0.83      1215\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1381   55  150]\n",
      " [ 172  837  106]\n",
      " [ 145   31 1039]]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_sequence_and_pad(text):\n",
    "    return pad_data(convert_to_sequences(text, \"\", False))\n",
    "\n",
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.821246169561\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.92      0.82      1542\n",
      "          1       0.87      0.81      0.84      1151\n",
      "          2       0.92      0.70      0.80      1223\n",
      "\n",
      "avg / total       0.84      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1426   68   48]\n",
      " [ 201  928   22]\n",
      " [ 288   73  862]]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_sequence_and_pad_and_filter_chars(text):\n",
    "    return pad_data(convert_to_sequences(text, \"~!@#$%^&*()_+`-=,./;'<>?:\\\"\", False))\n",
    "\n",
    "run(load_training_data, convert_to_sequence_and_pad_and_filter_chars, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.835801838611\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.92      0.84      1590\n",
      "          1       0.92      0.78      0.84      1160\n",
      "          2       0.87      0.78      0.82      1166\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1469   47   74]\n",
      " [ 204  900   56]\n",
      " [ 235   27  904]]\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_then_convert_to_sequence_and_pad(text):\n",
    "    return pad_data(convert_to_sequences(remove_stops(text), \"\", False))\n",
    "\n",
    "run(load_training_data, remove_stopwords_then_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "He cried aloud once ,  and a little later gave a gasp that was more terrible than a cry . \n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.853677221655\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.87      0.86      1580\n",
      "          1       0.92      0.80      0.86      1116\n",
      "          2       0.80      0.88      0.84      1220\n",
      "\n",
      "avg / total       0.86      0.85      0.85      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1369   44  167]\n",
      " [ 110  898  108]\n",
      " [ 113   31 1076]]\n"
     ]
    }
   ],
   "source": [
    "def convert_punctuation_to_words(text):\n",
    "    t = text.replace(\",\", \" , \")\n",
    "    t = t.replace(\".\", \" . \")\n",
    "    t = t.replace(\"'\", \" ' \")\n",
    "    t = t.replace(\";\", \" ; \")\n",
    "    t = t.replace(\":\", \" : \")\n",
    "    return t\n",
    "\n",
    "def make_punctuation_words_and_convert_to_sequence(text):\n",
    "    t = [convert_punctuation_to_words(x) for x in text]\n",
    "    return pad_data(convert_to_sequences(t, \"\", False))\n",
    "\n",
    "run(load_training_data, make_punctuation_words_and_convert_to_sequence, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.\n",
      "Finding nothing else , even gold , Superintendent abandoned attempts ; perplexed look occasionally steals countenance sits thinking desk .\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.837078651685\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.88      0.83      1517\n",
      "          1       0.90      0.79      0.84      1144\n",
      "          2       0.84      0.83      0.84      1255\n",
      "\n",
      "avg / total       0.84      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1328   59  130]\n",
      " [ 171  903   70]\n",
      " [ 168   40 1047]]\n"
     ]
    }
   ],
   "source": [
    "def punctuation_as_words_and_remove_stopwords(text):\n",
    "    t = convert_punctuation_to_words(text)\n",
    "    t = remove_stops(t)\n",
    "    return pad_data(convert_to_sequences(t, \"\", False))\n",
    "\n",
    "run(load_training_data, punctuation_as_words_and_remove_stopwords, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.82328907048\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.83      0.83      1559\n",
      "          1       0.87      0.79      0.83      1167\n",
      "          2       0.78      0.85      0.81      1190\n",
      "\n",
      "avg / total       0.83      0.82      0.82      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1291   83  185]\n",
      " [ 147  923   97]\n",
      " [ 131   49 1010]]\n"
     ]
    }
   ],
   "source": [
    "def create_simple_model_with_fewer_embedding_dims(input_dim):\n",
    "    return create_simple_model(input_dim, embedding_dims=10)\n",
    "\n",
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model_with_fewer_embedding_dims, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.812053115424\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.71      0.80      1594\n",
      "          1       0.79      0.89      0.83      1109\n",
      "          2       0.75      0.87      0.81      1213\n",
      "\n",
      "avg / total       0.82      0.81      0.81      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1138  183  273]\n",
      " [  41  985   83]\n",
      " [  72   84 1057]]\n"
     ]
    }
   ],
   "source": [
    "def create_simple_model_with_more_embedding_dims(input_dim):\n",
    "    return create_simple_model(input_dim, embedding_dims=30)\n",
    "\n",
    "run(load_training_data, convert_to_sequence_and_pad, create_simple_model_with_more_embedding_dims, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "The sky was serene; and, as I was unable to rest, I resolved to visit the spot where my poor William had been murdered.\n",
      "The sky be serene ; and , as I be unable to rest , I resolve to visit the spot where my poor William have be murder .\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.832992849847\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.85      0.84      1578\n",
      "          1       0.93      0.75      0.83      1127\n",
      "          2       0.78      0.89      0.83      1211\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1340   51  187]\n",
      " [ 157  848  122]\n",
      " [ 123   14 1074]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_texts_to_verbs(texts):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    return [\" \".join([lmtzr.lemmatize(word, \"v\") for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "\n",
    "def lemmatize_and_convert_to_sequence_and_pad(text):\n",
    "    t = lemmatize_texts_to_verbs(text)\n",
    "    return convert_to_sequence_and_pad(t)\n",
    "\n",
    "run(load_training_data, lemmatize_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "The sky was serene; and, as I was unable to rest, I resolved to visit the spot where my poor William had been murdered.\n",
      "The sky wa serene ; and , a I wa unable to rest , I resolved to visit the spot where my poor William had been murdered .\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.83835546476\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.91      0.85      1589\n",
      "          1       0.93      0.73      0.82      1161\n",
      "          2       0.83      0.85      0.84      1166\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1443   34  112]\n",
      " [ 218  847   96]\n",
      " [ 146   27  993]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_texts_to_nouns(texts):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    return [\" \".join([lmtzr.lemmatize(word, \"n\") for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "\n",
    "def lemmatize_and_convert_to_sequence_and_pad(text):\n",
    "    t = lemmatize_texts_to_nouns(text)\n",
    "    return convert_to_sequence_and_pad(t)\n",
    "\n",
    "run(load_training_data, lemmatize_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "The sky was serene; and, as I was unable to rest, I resolved to visit the spot where my poor William had been murdered.\n",
      "the sky wa seren ; and , as I wa unabl to rest , I resolv to visit the spot where my poor william had been murder .\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.843462717058\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.90      0.85      1606\n",
      "          1       0.88      0.85      0.86      1108\n",
      "          2       0.87      0.76      0.81      1202\n",
      "\n",
      "avg / total       0.85      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1446   73   87]\n",
      " [ 119  938   51]\n",
      " [ 224   59  919]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_texts(texts):\n",
    "    stmr = PorterStemmer()\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "    \n",
    "def stem_and_convert_to_sequence_and_pad(text):\n",
    "    t = stem_texts(text)\n",
    "    return convert_to_sequence_and_pad(t)\n",
    "\n",
    "run(load_training_data, stem_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "I strove to think that all this grandeur was but more glaring infamy, and that, by planting his gold enwoven flag beside my tarnished and tattered banner, he proclaimed not his superiority, but his debasement.\n",
      "i strove to think that all this grandeur was but more glare infami , and that , by plant his gold enwoven flag besid my tarnish and tatter banner , he proclaim not his superior , but his debas .\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.832992849847\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.81      0.83      1588\n",
      "          1       0.85      0.83      0.84      1136\n",
      "          2       0.79      0.86      0.82      1192\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1283  118  187]\n",
      " [  96  948   92]\n",
      " [ 111   50 1031]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stem_texts(texts):\n",
    "    stmr = SnowballStemmer(\"english\")\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]\n",
    "    \n",
    "def stem_and_convert_to_sequence_and_pad(text):\n",
    "    t = stem_texts(text)\n",
    "    return convert_to_sequence_and_pad(t)\n",
    "\n",
    "run(load_training_data, stem_and_convert_to_sequence_and_pad, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "def filter_words(sequences, filter_func):\n",
    "    return [[word for word in sequence if filter_func(word) is not False] for sequence in sequences]\n",
    "\n",
    "def create_infrequent_words_filter(num_to_keep):\n",
    "    def f(text):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        t = convert_to_word_sequence(text)\n",
    "        t = filter_words(t, lambda w: tokenizer.word_index[w] > num_to_keep)\n",
    "        return [[tokenizer.word_index[word] for word in seq] for seq in t]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.827630234934\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.91      0.84      1608\n",
      "          1       0.90      0.76      0.82      1152\n",
      "          2       0.84      0.78      0.81      1156\n",
      "\n",
      "avg / total       0.83      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1462   55   91]\n",
      " [ 197  872   83]\n",
      " [ 205   44  907]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(100)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.778855975485\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.75      0.78      1600\n",
      "          1       0.84      0.76      0.80      1111\n",
      "          2       0.70      0.84      0.76      1205\n",
      "\n",
      "avg / total       0.79      0.78      0.78      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1195  108  297]\n",
      " [ 130  841  140]\n",
      " [ 136   55 1014]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(1000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.516598569969\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.90      0.62      1574\n",
      "          1       0.72      0.34      0.46      1169\n",
      "          2       0.56      0.18      0.27      1173\n",
      "\n",
      "avg / total       0.57      0.52      0.47      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1414   82   78]\n",
      " [ 686  398   85]\n",
      " [ 886   76  211]]\n"
     ]
    }
   ],
   "source": [
    "def remove_most_frequent_words(text):\n",
    "    t = create_infrequent_words_filter(10000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_most_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "def create_frequent_words_filter(num_to_keep):\n",
    "    def f(text):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        t = convert_to_word_sequence(text)\n",
    "        t = filter_words(t, lambda w: tokenizer.word_index[w] <= num_to_keep)\n",
    "        return [[tokenizer.word_index[word] for word in seq] for seq in t]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.577374872319\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.65      1565\n",
      "          1       0.64      0.36      0.46      1152\n",
      "          2       0.58      0.54      0.56      1199\n",
      "\n",
      "avg / total       0.59      0.58      0.56      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1197  120  248]\n",
      " [ 510  416  226]\n",
      " [ 434  117  648]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(100)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.728038815117\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.73      0.74      1609\n",
      "          1       0.63      0.81      0.71      1068\n",
      "          2       0.81      0.66      0.73      1239\n",
      "\n",
      "avg / total       0.74      0.73      0.73      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1168  305  136]\n",
      " [ 145  861   62]\n",
      " [ 218  199  822]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(1000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.813329928498\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.86      0.82      1573\n",
      "          1       0.88      0.74      0.81      1131\n",
      "          2       0.81      0.82      0.82      1212\n",
      "\n",
      "avg / total       0.82      0.81      0.81      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1350   73  150]\n",
      " [ 207  836   88]\n",
      " [ 176   37  999]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(5000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.833758937692\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.91      0.84      1596\n",
      "          1       0.87      0.79      0.83      1098\n",
      "          2       0.91      0.78      0.84      1222\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1453   82   61]\n",
      " [ 198  863   37]\n",
      " [ 227   46  949]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(10000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "Training model\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.828907048008\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.92      0.83      1524\n",
      "          1       0.86      0.82      0.84      1154\n",
      "          2       0.92      0.73      0.81      1238\n",
      "\n",
      "avg / total       0.84      0.83      0.83      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1396   81   47]\n",
      " [ 175  943   36]\n",
      " [ 264   67  907]]\n"
     ]
    }
   ],
   "source": [
    "def remove_least_frequent_words(text):\n",
    "    t = create_frequent_words_filter(15000)(text)\n",
    "    return pad_data(t)\n",
    "\n",
    "run(load_training_data, remove_least_frequent_words, create_simple_model, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example knn classifier: https://stackoverflow.com/questions/42872425/text-classification-using-knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

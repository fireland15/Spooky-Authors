{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 2 - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, SimpleRNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set our hyper parameters that we will use elsewhere. Tbese were first set randomly and then tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hyper Parameters:\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "embedding_dims = 20\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 100\n",
    "epochs = 10\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in the data and change the author from initals to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "training data shape: (19579, 3)\n",
      "train labels:  [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "train text shape:  (19579,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the Data\n",
    "train = pd.read_csv('train.csv')\n",
    "print(train.head())\n",
    "print(\"training data shape:\", train.shape)\n",
    "\n",
    "#Transform Author and Split X and Y\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "labels = np.array([a2c[a] for a in train.author])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "print(\"train labels: \", labels)\n",
    "text = train[\"text\"]\n",
    "print(\"train text shape: \", text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the data and pad the sequence so that it is the same size. We did not remove stop words or do other pre-processing since we found that it reduce accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "tokenized:  [26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "max doc length:  861\n",
      "Pad sequences (samples x time)\n",
      "padded docs shape: (19579, 861)\n"
     ]
    }
   ],
   "source": [
    "#Pre-Processing\n",
    "\n",
    "#removing stopwords reduces accuracy\n",
    "\n",
    "#Tokenize\n",
    "tokenizer = Tokenizer(lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(text);\n",
    "docs = tokenizer.texts_to_sequences(text);\n",
    "print(\"text: \", text[0])\n",
    "print(\"tokenized: \", docs[0])\n",
    "maxlen = np.amax([len(x) for x in docs], axis=0)\n",
    "print(\"max doc length: \", maxlen)\n",
    "\n",
    "#Pad to all doc are the same length\n",
    "print('Pad sequences (samples x time)')\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "print('padded docs shape:', docs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the testing and training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 861, 20)           518880    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 861, 100)          2100      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 861, 100)          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 541,383\n",
      "Trainable params: 541,383\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 221s 14ms/step - loss: 0.5427 - acc: 0.7221 - val_loss: 0.4920 - val_acc: 0.7506\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 214s 14ms/step - loss: 0.3286 - acc: 0.8575 - val_loss: 0.3459 - val_acc: 0.8556\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 220s 14ms/step - loss: 0.1626 - acc: 0.9365 - val_loss: 0.3573 - val_acc: 0.8688\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 225s 14ms/step - loss: 0.0942 - acc: 0.9654 - val_loss: 0.3644 - val_acc: 0.8755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0f9da07f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#Embedding layer\n",
    "input_dim = np.max(docs) + 1\n",
    "\n",
    "model.add(Embedding(input_dim,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(hidden_dims, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#RNN Layer\n",
    "model.add(SimpleRNN(100, activation=\"relu\"))\n",
    "\n",
    "#SoftMax Layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Train and test the model\n",
    "model.fit(x_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=patience, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems very promising our accuracy was 87%, which is the highest we have run into so far. This suggests that an RNN is a sutiable model for this text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Kaggle test results - test_results.csv\n",
      "Pad sequences (samples x time)\n",
      "padded docs shape: (8392, 861)\n",
      "(8392, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Kaggle test results - test_results.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data = test_data[\"text\"]\n",
    "\n",
    "docs = tokenizer.texts_to_sequences(data);\n",
    "\n",
    "#Pad to all doc are the same length\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "\n",
    "out = pd.DataFrame(model.predict(docs))\n",
    "ids = test_data[\"id\"]\n",
    "data_to_write = pd.concat([ids, out], axis=1)\n",
    "data_to_write.columns =[\"id\", \"EAP\", \"HPL\", \"MWS\"]\n",
    "print(data_to_write.shape)\n",
    "data_to_write.to_csv(\"test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Kaggle results were good, the best that we got during the competition. Our loss was 0.577, and it placed us number # 728 on the public leader board. This suggest that RNN is a good method for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 1: Training using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and downloads\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dropout, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful functions that we are defining to make our lives easier. These include some pre-processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "    labels = np.array([a2c[a] for a in data.author])\n",
    "    labels = to_categorical(labels)\n",
    "    return labels\n",
    "\n",
    "def get_text_only(data):\n",
    "    return data[\"text\"]\n",
    "\n",
    "def calc_metrics(x, y_true, y_pred):\n",
    "    return classification_report(y_true, y_pred)\n",
    "\n",
    "def calc_confusion_matrix(y_true, y_pred):\n",
    "    return confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))\n",
    "\n",
    "# converts the sentences in text into a sequence of numbers\n",
    "# input: an array of strings\n",
    "# output: an array of  sequences of numbers\n",
    "def convert_to_sequences_remove_chars(text):\n",
    "    tokenizer = Tokenizer(split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# converts the sentences in text into a sequence of numbers\n",
    "# input: an array of strings\n",
    "# output: an array of  sequences of numbers\n",
    "def convert_to_sequences_leave_chars(text):\n",
    "    tokenizer = Tokenizer(filters=\"\", split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# Pads the input arrays to be of equal length\n",
    "# input: An array of sequences of numbers\n",
    "# output: an array of sequences of numbers\n",
    "def pad_data(text):\n",
    "    maxlen = np.amax([len(x) for x in text], axis=0)\n",
    "    return pad_sequences(sequences=text, maxlen=maxlen) \n",
    "\n",
    "# removes stop words from the sentences in text\n",
    "# input: an array of strings\n",
    "# output: an array of strings\n",
    "def remove_stops(text):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [\" \".join([word for word in nltk.word_tokenize(words) if word not in stops]) for words in text]\n",
    "\n",
    "# We had seen a submission where the person treated punctuation as distinct words, we thought this would be worth trying\n",
    "#  seeing as some authors may have different patterns of punctuation\n",
    "# input: an array of strings\n",
    "# output: an array of strings\n",
    "def convert_punctuation_to_words(texts):\n",
    "    chars = \"~!@#$%^&*()_+`-=,./;'<>?:\\\"\"\n",
    "  \n",
    "    for c in chars:\n",
    "        texts = [text.replace(c, \" \" + c + \" \") for text in texts]\n",
    "    return texts\n",
    "\n",
    "# Uses the Porter stemmer to stem each word in the texts\n",
    "# input: an array of strings\n",
    "# output: an array of strings\n",
    "def stem_texts_porter(texts):\n",
    "    stmr = PorterStemmer()\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the training data\n",
    "def load_training_data():\n",
    "    return pd.read_csv(\"train.csv\")\n",
    "def load_test_data():\n",
    "    return pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function that will run our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to execute a machine learning pipeline\n",
    "# load_func : the function that loads training data\n",
    "# preprocessing_func_arr : an array of functions that preprocess the data. Executed in order and the output feeds into the input of the next\n",
    "# create_model_func : a function that creates a model to train and test.\n",
    "def run(load_func, preprocess_func_arr, create_model_func, verbosity=2, preprocess_debug=False):\n",
    "    print(\"Loading data\")\n",
    "    full_data = load_func()\n",
    "    \n",
    "    print(\"Getting labels\")\n",
    "    labels = make_labels(full_data)    \n",
    "    \n",
    "    print(\"Preprocessing\")\n",
    "    data = get_text_only(full_data)\n",
    "    for func in preprocess_func_arr:\n",
    "        data = func(data)\n",
    "        if preprocess_debug is True:\n",
    "            print(data[0])\n",
    "    \n",
    "    input_dim = max([max(x) for x in data]) + 1\n",
    "\n",
    "    print(\"Creating model\")\n",
    "    model = create_model_func(input_dim)\n",
    "    \n",
    "    print(\"Model Summary\", model.summary())\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "    \n",
    "    print(\"Training model\")\n",
    "    model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=64,\n",
    "                 verbose=verbosity,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "    print(\"Testing model\")\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    y_pred = to_categorical(y_pred, num_classes=3)  \n",
    "    \n",
    "    print(\"Test results\")  \n",
    "    print(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    print(\"metrics\")\n",
    "    print(calc_metrics(x_test, y_test, y_pred))\n",
    "    print(\"confusion matrix\")\n",
    "    print(calc_confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(\"Generating Kaggle test results - test_results.csv\")\n",
    "    test_data = load_test_data()\n",
    "    data = test_data[\"text\"]\n",
    "    for func in preprocess_func_arr:\n",
    "        data = func(data)\n",
    "    out = pd.DataFrame(model.predict(data))\n",
    "    ids = test_data[\"id\"]\n",
    "    data_to_write = pd.concat([ids, out], axis=1)\n",
    "    data_to_write.columns =[\"id\", \"EAP\", \"HPL\", \"MWS\"]\n",
    "    print(data_to_write.shape)\n",
    "    data_to_write.to_csv(\"test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define some different models. We created a few here, but only one of them was any good. We found that the embedding and pooling model was the only one worth while. We explore convolutional networks more in submission 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the best performing of the three models, it has the lowest loss of all three.\n",
    "# All three have similar accuracies.\n",
    "def embedding_and_pooling_model(input_dim, embedding_dims=500, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def embedding_conv1d_pooling_model(input_dim, embedding_dims=100, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(Conv1D(16, 8, activation=\"relu\"))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def deeper_with_multiple_convolutions(input_dim, embedding_dims=100, optimizer=RMSprop(lr=0.003)):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(Conv1D(128, 8, activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128, 8, activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Forrest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# converts the sentences in text into a sequence of numbers\n",
    "# input: an array of strings\n",
    "# output: an array of  sequences of numbers\n",
    "def convert_to_sequences_remove_chars(text):\n",
    "    tokenizer = Tokenizer(split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# converts the sentences in text into a sequence of numbers\n",
    "# input: an array of strings\n",
    "# output: an array of  sequences of numbers\n",
    "def convert_to_sequences_leave_chars(text):\n",
    "    tokenizer = Tokenizer(filters=\"\", split=\" \", char_level=False)\n",
    "    tokenizer.fit_on_texts(text);\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pads the input arrays to be of equal length\n",
    "# input: An array of sequences of numbers\n",
    "# output: an array of sequences of numbers\n",
    "def pad_data(text):\n",
    "    maxlen = np.amax([len(x) for x in text], axis=0)\n",
    "    return pad_sequences(sequences=text, maxlen=maxlen) \n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# removes stop words from the sentences in text\n",
    "# input: an array of strings\n",
    "# output: an array of strings\n",
    "def remove_stops(text):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [\" \".join([word for word in nltk.word_tokenize(words) if word not in stops]) for words in text]\n",
    "\n",
    "# We had seen a submission where the person treated punctuation as distinct words, we thought this would be worth trying\n",
    "#  seeing as some authors may have different patterns of punctuation\n",
    "# input: an array of strings\n",
    "# output: an array of strings\n",
    "def convert_punctuation_to_words(texts):\n",
    "    chars = \"~!@#$%^&*()_+`-=,./;'<>?:\\\"\"\n",
    "  \n",
    "    for c in chars:\n",
    "        texts = [text.replace(c, \" \" + c + \" \") for text in texts]\n",
    "    return texts\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Uses the Porter stemmer to stem each word in the texts\n",
    "# input: an array of strings\n",
    "# output: an array of strings\n",
    "def stem_texts_porter(texts):\n",
    "    stmr = PorterStemmer()\n",
    "    return [\" \".join([stmr.stem(word) for word in nltk.word_tokenize(text)]) for text in texts]#Finally, we run our model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Getting labels\n",
      "Preprocessing\n",
      "Creating model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 500)         12535000  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 1503      \n",
      "=================================================================\n",
      "Total params: 12,536,503\n",
      "Trainable params: 12,536,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Summary None\n",
      "Training model\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/64\n",
      " - 19s - loss: 1.0700 - acc: 0.4189 - val_loss: 1.0183 - val_acc: 0.4257\n",
      "Epoch 2/64\n",
      " - 18s - loss: 0.8634 - acc: 0.6572 - val_loss: 0.7371 - val_acc: 0.7561\n",
      "Epoch 3/64\n",
      " - 19s - loss: 0.5809 - acc: 0.8136 - val_loss: 0.5667 - val_acc: 0.7903\n",
      "Epoch 4/64\n",
      " - 19s - loss: 0.4196 - acc: 0.8675 - val_loss: 0.4910 - val_acc: 0.8174\n",
      "Epoch 5/64\n",
      " - 19s - loss: 0.3353 - acc: 0.8885 - val_loss: 0.4607 - val_acc: 0.8210\n",
      "Epoch 6/64\n",
      " - 18s - loss: 0.2769 - acc: 0.9087 - val_loss: 0.4428 - val_acc: 0.8274\n",
      "Epoch 7/64\n",
      " - 19s - loss: 0.2258 - acc: 0.9275 - val_loss: 0.4248 - val_acc: 0.8343\n",
      "Epoch 8/64\n",
      " - 19s - loss: 0.1942 - acc: 0.9365 - val_loss: 0.4321 - val_acc: 0.8307\n",
      "Epoch 9/64\n",
      " - 18s - loss: 0.1662 - acc: 0.9455 - val_loss: 0.4130 - val_acc: 0.8430\n",
      "Epoch 10/64\n",
      " - 19s - loss: 0.1446 - acc: 0.9524 - val_loss: 0.4430 - val_acc: 0.8292\n",
      "Epoch 11/64\n",
      " - 19s - loss: 0.1271 - acc: 0.9587 - val_loss: 0.4347 - val_acc: 0.8368\n",
      "Training complete\n",
      "Testing model\n",
      "Test results\n",
      "accuracy 0.83682328907\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.83      0.84      1579\n",
      "          1       0.82      0.86      0.84      1132\n",
      "          2       0.83      0.83      0.83      1205\n",
      "\n",
      "avg / total       0.84      0.84      0.84      3916\n",
      "\n",
      "confusion matrix\n",
      "[[1305  134  140]\n",
      " [  92  972   68]\n",
      " [ 126   79 1000]]\n",
      "Generating Kaggle test results - test_results.csv\n",
      "(8392, 4)\n"
     ]
    }
   ],
   "source": [
    "preprocessors = [\n",
    "    convert_punctuation_to_words,\n",
    "    remove_stops,\n",
    "    convert_to_sequences_remove_chars,\n",
    "    pad_data\n",
    "]\n",
    "\n",
    "run(load_training_data, preprocessors, embedding_and_pooling_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was semi-sucessful. Putting it up on the Kaggle site we scored 2.55803, which put us in 1054th place, which isnt very good. However, this is a pretty simple model with no convolutional layers, but a lot of pre-processing. Throughout this competetion, we found that pre-processing was less important than having a good model. This could be becuase our pre-processes wasnt affecting the things that the networks were learning on. Since these networks are so black box, it is hard to tell if that was the case. Another reason is that we just were not doing the right sort of pre-processing and were not extracting feature that were actually important. From this jumping off point we continued on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

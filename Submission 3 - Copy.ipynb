{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hyper Parameters:\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "embedding_dims = 20\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "training data shape: (19579, 3)\n",
      "train labels:  [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "train text shape:  (19579,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the Data\n",
    "train = pd.read_csv('train.csv')\n",
    "print(train.head())\n",
    "print(\"training data shape:\", train.shape)\n",
    "\n",
    "#Transform Author and Split X and Y\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "labels = np.array([a2c[a] for a in train.author])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "print(\"train labels: \", labels)\n",
    "text = train[\"text\"]\n",
    "print(\"train text shape: \", text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Beth &\n",
      "[nltk_data]     Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "text:  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "tokenized:  [26, 8207, 142, 1331, 31, 37, 272, 2, 12060, 1, 5088, 2, 10, 22483, 16, 6, 75, 160, 45, 22484, 3, 341, 4, 1, 274, 1996, 6, 302, 1062, 121, 123, 886, 2, 1, 22485, 39, 1332, 6237, 88, 1, 2401]\n",
      "max doc length:  861\n",
      "Pad sequences (samples x time)\n",
      "padded docs shape: (19579, 861)\n"
     ]
    }
   ],
   "source": [
    "#Pre-Processing\n",
    "\n",
    "#removing stopwords reduces accuracy\n",
    "\n",
    "#Tokenize\n",
    "tokenizer = Tokenizer(filters=\"\", lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(text);\n",
    "docs = tokenizer.texts_to_sequences(text);\n",
    "print(\"text: \", text[0])\n",
    "print(\"tokenized: \", docs[0])\n",
    "maxlen = np.amax([len(x) for x in docs], axis=0)\n",
    "print(\"max doc length: \", maxlen)\n",
    "\n",
    "#Pad to all doc are the same length\n",
    "print('Pad sequences (samples x time)')\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "print('padded docs shape:', docs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the testing and training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 8s 536us/step - loss: 0.5566 - acc: 0.7179 - val_loss: 0.4122 - val_acc: 0.8139\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 8s 503us/step - loss: 0.2776 - acc: 0.8860 - val_loss: 0.3273 - val_acc: 0.8542\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 8s 523us/step - loss: 0.1226 - acc: 0.9538 - val_loss: 0.3450 - val_acc: 0.8618\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 8s 500us/step - loss: 0.0569 - acc: 0.9802 - val_loss: 0.4181 - val_acc: 0.8467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b840c39b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#Embedding layer\n",
    "input_dim = np.max(docs) + 1\n",
    "\n",
    "model.add(Embedding(input_dim,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "# Max Pooling Layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(hidden_dims+200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#SoftMax Layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train and test the model\n",
    "model.fit(x_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=patience, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "training data shape: (19579, 3)\n",
      "train labels:  [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "train text shape:  (19579,)\n",
      "text:  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "tokenized:  [26, 8207, 142, 1331, 31, 37, 272, 2, 12060, 1, 5088, 2, 10, 22483, 16, 6, 75, 160, 45, 22484, 3, 341, 4, 1, 274, 1996, 6, 302, 1062, 121, 123, 886, 2, 1, 22485, 39, 1332, 6237, 88, 1, 2401]\n",
      "max doc length:  861\n",
      "Pad sequences (samples x time)\n",
      "padded docs shape: (19579, 861)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "embedding_dims = 20\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "print(train.head())\n",
    "print(\"training data shape:\", train.shape) # three columns are id, text, author\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "labels = np.array([a2c[a] for a in train.author])\n",
    "labels = to_categorical(labels)\n",
    "print(\"train labels: \", labels)\n",
    "\n",
    "text = train[\"text\"]\n",
    "print(\"train text shape: \", text.shape)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\", lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(text);\n",
    "docs = tokenizer.texts_to_sequences(text);\n",
    "print(\"text: \", text[0])\n",
    "print(\"tokenized: \", docs[0])\n",
    "\n",
    "\n",
    "maxlen = np.amax([len(x) for x in docs], axis=0)\n",
    "print(\"max doc length: \", maxlen)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "print('padded docs shape:', docs.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 14s 891us/step - loss: 0.4917 - acc: 0.7605 - val_loss: 0.3424 - val_acc: 0.8532\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 12s 756us/step - loss: 0.2334 - acc: 0.9068 - val_loss: 0.3191 - val_acc: 0.8606\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 11s 731us/step - loss: 0.0978 - acc: 0.9654 - val_loss: 0.3462 - val_acc: 0.8704\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 11s 724us/step - loss: 0.0428 - acc: 0.9853 - val_loss: 0.4474 - val_acc: 0.8637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b44918c18>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "input_dim = np.max(docs) + 1\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=10,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

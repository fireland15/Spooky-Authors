{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "embedding_dims = 20\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "print(train.head())\n",
    "print(\"training data shape:\", train.shape) # three columns are id, text, author\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "labels = np.array([a2c[a] for a in train.author])\n",
    "labels = to_categorical(labels)\n",
    "print(\"train labels: \", labels)\n",
    "\n",
    "text = train[\"text\"]\n",
    "print(\"train text shape: \", text.shape)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\", lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(text);\n",
    "docs = tokenizer.texts_to_sequences(text);\n",
    "print(\"text: \", text[0])\n",
    "print(\"tokenized: \", docs[0])\n",
    "\n",
    "\n",
    "maxlen = np.amax([len(x) for x in docs], axis=0)\n",
    "print(\"max doc length: \", maxlen)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "print('padded docs shape:', docs.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "input_dim = np.max(docs) + 1\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=10,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

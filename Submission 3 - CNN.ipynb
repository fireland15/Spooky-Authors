{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 3: CNN\n",
    "\n",
    "For our third submission, we designed a covolutional neural network that includes 2 hidden layers. Why we chose this configuration follows:\n",
    "\n",
    "The first step is to import everything that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing, we set some hyper parameters. These hyper parameters were first set randomly, then through running the model over and over again, we found that these paramters produces a good result. While these are most likely not the optimal configuations for this network, we found that they were close and provided a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hyper Parameters:\n",
    "max_features = 5000\n",
    "batch_size = 32\n",
    "embedding_dims = 20\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to import the data. We import just the training data in this step as that is what we will be using to train the model. After checking that our data looks as expected. We transform the author initals into a 0,1 or 2. This become our labels, or y of the training data. The text becomes our x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "training data shape: (19579, 3)\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "training data shape: (19579, 3)\n",
      "train labels:  [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "train text shape:  (19579,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the Data\n",
    "train = pd.read_csv('train.csv')\n",
    "print(train.head())\n",
    "print(\"training data shape:\", train.shape)\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "print(train.head())\n",
    "print(\"training data shape:\", train.shape)\n",
    "\n",
    "#Transform Author and Split X and Y\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "labels = np.array([a2c[a] for a in train.author])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "print(\"train labels: \", labels)\n",
    "text = train[\"text\"]\n",
    "print(\"train text shape: \", text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to pre process out data. Intrestingly enough, all of the pre-processing we tried made the result worse! One thing that we tried was removing stop words. This did not increase the accuracy of our model, instead it reduced it. There is a reason that we think this may have happened. Most of the text is written in an older style of English than what we use today. Removing stop words that are used in modern language is only helpful if those stopwords are present. Addititonally those stop words indicate a more causal form of writing and could have distinguished one author from another. We also tried to change the tokenizer to ignore case, remove puncuation ect. This did not increase the accuracy of our model. Finally, we pad the text so that all of the inputs are the same lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "tokenized:  [26, 8207, 142, 1331, 31, 37, 272, 2, 12060, 1, 5088, 2, 10, 22483, 16, 6, 75, 160, 45, 22484, 3, 341, 4, 1, 274, 1996, 6, 302, 1062, 121, 123, 886, 2, 1, 22485, 39, 1332, 6237, 88, 1, 2401]\n",
      "max doc length:  861\n",
      "Pad sequences (samples x time)\n",
      "padded docs shape: (19579, 861)\n"
     ]
    }
   ],
   "source": [
    "#Pre-Processing\n",
    "\n",
    "#removing stopwords reduces accuracy\n",
    "\n",
    "#Tokenize\n",
    "tokenizer = Tokenizer(filters=\"\", lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(text);\n",
    "docs = tokenizer.texts_to_sequences(text);\n",
    "print(\"text: \", text[0])\n",
    "print(\"tokenized: \", docs[0])\n",
    "maxlen = np.amax([len(x) for x in docs], axis=0)\n",
    "print(\"max doc length: \", maxlen)\n",
    "\n",
    "#Pad to all doc are the same length\n",
    "print('Pad sequences (samples x time)')\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "print('padded docs shape:', docs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the training data into test and training data so we can see the accuracy of our model using label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the testing and training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are at the step to create the model. To begin we apply an Embedding layer. To prevent overtraining the model, we add a drop-out layer of 0.2. Then we add a convolutional layer with 250 filters. After that we added two hidden layers and the a softmax layer. The configuration of this model was trial and error as we tried different conovlutional layers and hidden layers. We found that the resulting configuration produces the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 10s 644us/step - loss: 0.5354 - acc: 0.7325 - val_loss: 0.3763 - val_acc: 0.8355\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 8s 513us/step - loss: 0.2608 - acc: 0.8924 - val_loss: 0.3174 - val_acc: 0.8676\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 8s 510us/step - loss: 0.1131 - acc: 0.9570 - val_loss: 0.3539 - val_acc: 0.8607\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 8s 503us/step - loss: 0.0544 - acc: 0.9808 - val_loss: 0.4197 - val_acc: 0.8560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x209096b4c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#Embedding layer\n",
    "input_dim = np.max(docs) + 1\n",
    "\n",
    "model.add(Embedding(input_dim,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "# Max Pooling Layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(hidden_dims+200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#SoftMax Layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train and test the model\n",
    "model.fit(x_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=patience, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Kaggle test results - test_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Kaggle test results - test_results.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data = test_data[\"text\"]\n",
    "\n",
    "docs = tokenizer.texts_to_sequences(data);\n",
    "\n",
    "#Pad to all doc are the same length\n",
    "docs = sequence.pad_sequences(docs, maxlen=maxlen)\n",
    "\n",
    "out = pd.DataFrame(model.predict(docs))\n",
    "ids = test_data[\"id\"]\n",
    "data_to_write = pd.concat([ids, out], axis=1)\n",
    "data_to_write.columns =[\"id\", \"EAP\", \"HPL\", \"MWS\"]\n",
    "data_to_write.to_csv(\"test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
